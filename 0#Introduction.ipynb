{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: NN's and Deep Learning Theory\n",
    "\n",
    "Deep Learning is about Neural Networks.\n",
    "\n",
    "#### NN: A system modelled inspired by the human brain and nervous system.\n",
    "\n",
    "The structure of NN is like any other kind of networks, there is an interconnected web of nodes \"Neurons\", and the edges that join them together.\n",
    "\n",
    "The main is to receive inputs and perform complex calculations, and use the output to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher score from the output means it has a high level of confidence!\n",
    "\n",
    "Forward prop is a neural net's way of classiying a set of inputs. Starts with the inputs to Output (working forward)\n",
    "\n",
    "Backpropagation normally is used to train the data. (Output - Input)\n",
    "\n",
    "Weight = Edge.\n",
    "Bias = Node.\n",
    "And the prediction accuracy of NN depends on its weights and biases.\n",
    "\n",
    "#### COST = Generated Output (predicted) - Actual Output\n",
    "And the point of training is to make the cost as small as possible, accross millions of training examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Nets = Pattern Recognition\n",
    "\n",
    "Text Processing -> RNTN, Recurrent Net\n",
    "\n",
    "Image Recognition -> DBN, Convolutional Net\n",
    "\n",
    "Object Recognition -> RNTN, Convolutional Net\n",
    "\n",
    "Speech Recognition -> Deep Learning, Recurrent Net\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Problem\n",
    "When training with backpropagation, you run into a fundamental problem called the vanishing / exploding gradient.\n",
    "When that happens, training takes too long and the accuracy really suffers.\n",
    "\n",
    "#### GRADIENT = rate at which cost changes with respect to weight or bias\n",
    "##### Higher Gradient, Quicker training\n",
    "Total Gradients in a node = the number of gradient weight + gradient bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restricted Boltzman Machines\n",
    "\n",
    "The breakthrough that allowed deep nets to combat the vanishing gradient problem? first, involves the RBM.\n",
    "An algorith that can automatically detect the inherent patterns in data by ###Reconstructing the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
